{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 자연어 처리\n",
    "# 토픽 모델링 : 잠재 의미 분석(LSA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토픽 모델링(Topic Modeling)이란 문서 집합에 숨어 있는 '주제'를 찾아내는 텍스트 마이닝기법 중 하나이다. 문서에 함축되어 있는 주요 주제를 효과적으로 찾아낼 수 있다. \n",
    "\n",
    "토픽 모델링에 자주 사용되는 기법은 LSA(Latent Semantic Analysis)와 LDA(Latent Dirichlet Allocation)이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기존의 카운트 기반 벡터화, TF-IDF로 얻은 문서-단어 행렬은 단어의 빈도수를 이용하여 행렬을 만들었다. 하지만 단어의 빈도수만으로는 주제를 파악하기 힘들다. 빈도로 단어의 의미를 정확히 고려하지 못하기 때문이다.이에 대한 방안으로 나온 것이 문서의 잠재된 의미를 찾아낸느 잠재 의미 분석(LSA)이다. LSA는 SVD를 활용하여 문서에 주제를 찾아내는 것이기 때문에 SVD를 이해했으면 LSA 이해는 금방이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSA 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "dataset = fetch_20newsgroups(shuffle=True,random_state=98, remove=('headers','footers','quotes'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 형태 확인 :  dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n"
     ]
    }
   ],
   "source": [
    "print('데이터 형태 확인 : ',dataset.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fetch_20newsgroups : 20개의 토픽을 가지 11,314개의 뉴스 기사 데이터 셋\n",
    "documents = dataset.data\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSuperficially a good answer, but it isn\\'t that simple.  An awful lot of the\\nstarvation and poverty in the world is directly caused by the economic\\npolicies of the Western countries, as well as by the diet of the typical\\nWesterner.  For instance, some third-world countries with terrible\\nmalnutrition problems export all the soya they can produce -- so that it can\\nbe fed to cattle in the US, to make tender juicy steaks and burgers.  They\\nhave to do this to get money to pay the interest on the crippling bank loans\\nwe encouraged them to take out.  Fund-raising for Ethiopia is a truly bizarre\\nidea; instead, we ought to stop bleeding them for every penny they\\'ve got.\\n\\nPerhaps it\\'s more accurate to say that there\\'s a Western ethic against\\nWestern infanticide.  All the evidence suggests that so long as the children\\nare dying in the Third World, we couldn\\'t give a shit.  And that goes for the\\nsupposed \"Pro-Life\" movement, too.  They could save far more lives by\\nfighting against Third World debt than they will by fighting against\\nabortion.  Hell, if they\\'re only interested in fetuses, they could save more\\nof those by fighting for human rights in China.\\n\\nAnd besides, Suzanne\\'s answer implies that non-Western countries lack this\\nethic against infanticide.  Apart from China, with its policy of mandatory\\nforced abortion in Tibet, I don\\'t believe this to be the case.\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Various posts about shafties can't do wheelies:\\n\\n\\nUh, folks, the shaft doesn't have diddleysquatpoop to do with it. I can get\\nthe front wheel off the ground on my /5, ferchrissake!\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[11313]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같은 문서 형태를 가진 문서는 총 11,314개 이다. 문서의 토픽은 target_names에 저장되어 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSA 전 데이터 전처리 : 알파벳 이외의 문자 제거, 길이가 3 이하인 문자도 제거, 대소문자 구분 없애기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.DataFrame({'document':documents})\n",
    "# 알파벳 이외 문자 제거\n",
    "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z#]\",\" \")\n",
    "#길이 3 이하 문자 제거\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "# 소문자 바꾸기\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>clean_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nSuperficially a good answer, but it isn't th...</td>\n",
       "      <td>superficially good answer that simple awful st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OHIO HOUSE OF REPRESENTATIVE  TUEDAY, APRIL 6,...</td>\n",
       "      <td>ohio house representative tueday april represe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n +------------------------------------------...</td>\n",
       "      <td>kevin marshall operational support motorola ec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Don A.B. Lindbergh meinte am 15.04.93\\nzum The...</td>\n",
       "      <td>lindbergh meinte thema diamond mouse cursor an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\n\\n     I HEARTILY agree.  Now that the BAT...</td>\n",
       "      <td>heartily agree that batf warrant been unsealed...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            document  \\\n",
       "0  \\nSuperficially a good answer, but it isn't th...   \n",
       "1  OHIO HOUSE OF REPRESENTATIVE  TUEDAY, APRIL 6,...   \n",
       "2  \\n +------------------------------------------...   \n",
       "3  Don A.B. Lindbergh meinte am 15.04.93\\nzum The...   \n",
       "4  \\n\\n\\n     I HEARTILY agree.  Now that the BAT...   \n",
       "\n",
       "                                           clean_doc  \n",
       "0  superficially good answer that simple awful st...  \n",
       "1  ohio house representative tueday april represe...  \n",
       "2  kevin marshall operational support motorola ec...  \n",
       "3  lindbergh meinte thema diamond mouse cursor an...  \n",
       "4  heartily agree that batf warrant been unsealed...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "doc TF-IDF 벡터화 1,000 단어까지만 벡터화 진행(max_features = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 1000)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english',max_features=1000,max_df=0.5,smooth_idf=True)\n",
    "\n",
    "X = vectorizer.fit_transform(news_df['clean_doc'])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11314개 문서를 1,000 단어를 활용하여 문서-단어 행렬 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 1000)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD #토픽모델링\n",
    "\n",
    "svd_model = TruncatedSVD(n_components=20, algorithm = 'randomized', n_iter=100, random_state=12)\n",
    "svd_model.fit(X)\n",
    "svd_model.components_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "뉴스 토픽이 총 20개 이므로 n_components는 20으로 설정, Truncated SVD 분해시 상위 20개 특이값만 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17.15952833,  9.93882749,  8.17139855,  7.92032011,  7.62377374,\n",
       "        7.5257242 ,  7.25096862,  7.00623237,  6.88289372,  6.85602044,\n",
       "        6.68476301,  6.56045782,  6.52895929,  6.42222944,  6.33939436,\n",
       "        6.21686249,  6.17477882,  6.09487639,  6.00247117,  5.90654237])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_model.singular_values_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names()\n",
    "len(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:  ['just', 'like', 'know', 'people', 'think', 'does', 'good', 'time']\n",
      "Topic 2:  ['thanks', 'windows', 'card', 'drive', 'mail', 'file', 'advance', 'files']\n",
      "Topic 3:  ['game', 'team', 'year', 'games', 'drive', 'season', 'good', 'players']\n",
      "Topic 4:  ['drive', 'scsi', 'disk', 'hard', 'problem', 'drives', 'just', 'card']\n",
      "Topic 5:  ['drive', 'know', 'thanks', 'does', 'just', 'scsi', 'drives', 'hard']\n",
      "Topic 6:  ['just', 'like', 'windows', 'know', 'does', 'window', 'file', 'think']\n",
      "Topic 7:  ['just', 'like', 'mail', 'bike', 'thanks', 'chip', 'space', 'email']\n",
      "Topic 8:  ['does', 'know', 'chip', 'like', 'card', 'clipper', 'encryption', 'government']\n",
      "Topic 9:  ['like', 'card', 'sale', 'video', 'offer', 'jesus', 'good', 'price']\n",
      "Topic 10:  ['like', 'drive', 'file', 'files', 'sounds', 'program', 'window', 'space']\n",
      "Topic 11:  ['people', 'like', 'thanks', 'card', 'government', 'windows', 'right', 'think']\n",
      "Topic 12:  ['think', 'good', 'thanks', 'need', 'chip', 'know', 'really', 'bike']\n",
      "Topic 13:  ['think', 'does', 'just', 'mail', 'like', 'game', 'file', 'chip']\n",
      "Topic 14:  ['know', 'good', 'people', 'windows', 'file', 'sale', 'files', 'price']\n",
      "Topic 15:  ['space', 'know', 'think', 'nasa', 'card', 'year', 'shuttle', 'article']\n",
      "Topic 16:  ['does', 'israel', 'think', 'right', 'israeli', 'sale', 'jews', 'window']\n",
      "Topic 17:  ['good', 'space', 'card', 'does', 'thanks', 'year', 'like', 'nasa']\n",
      "Topic 18:  ['people', 'does', 'window', 'problem', 'space', 'using', 'work', 'server']\n",
      "Topic 19:  ['right', 'bike', 'time', 'windows', 'space', 'does', 'file', 'thanks']\n",
      "Topic 20:  ['file', 'problem', 'files', 'need', 'time', 'card', 'game', 'people']\n"
     ]
    }
   ],
   "source": [
    "n = 8 #주요 단어 8개 출력\n",
    "components = svd_model.components_\n",
    "for index, topic in enumerate(components):\n",
    "    print('Topic %d: '%(index+1),[terms[i] for i in topic.argsort()[: -n -1:-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic 3을 스포츠 관련 기사일 것이고, Topic 18은 컴퓨터 관련 기사일 것이다. 불용어를 제외하면 더 정확한 결과가 나올 수 있을듯 하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.target_names # target과 연결해보기"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
